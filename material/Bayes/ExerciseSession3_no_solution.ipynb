{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from  scipy.stats import norm\n",
    "from  scipy.stats import binom\n",
    "from scipy.stats import beta\n",
    "from  scipy.stats import uniform\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "import pystan\n",
    "from pystan.constants import MAX_UINT\n",
    "\n",
    "%run ../tools.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "\n",
    "The goal of this exercise is to implement the Metropolis-Hastings method as seen in Lesson 5. Consider a variable y_i representing the number of success of an experiment i repeated n times. Now consider a vector Y containing the number of success of N independent experiments y_i. We assume that each experiment_i share a common probability of success $\\theta = 0.3$. Therefore we can generate the following data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29 37 32 28 30 34 22 31 27 31]\n"
     ]
    }
   ],
   "source": [
    "N = 100\n",
    "n = 100\n",
    "theta = 0.3\n",
    "\n",
    "Y = binom.rvs(n, theta, size=N)\n",
    "    \n",
    "print(Y[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1) We will assume a prior Beta distribution with parameters a=0.5 and b=0.5 for $\\theta$. Write the data likelihood p(Y|$\\theta$) and the posterior distribution p($\\theta$ | Y).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2) Estimate the posterior distirbution p($\\theta$|Y) using the Metropolis-Hastings algorithm.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def proposal(prop_mu, prop_sigma):\n",
    "    return scipy.stats.norm.rvs(loc = prop_mu, scale = prop_sigma, size = 1)\n",
    "\n",
    "def log_likelihood(param):\n",
    "    return np.sum([binom.logpmf(y_i,n,param) for y_i in Y])\n",
    "\n",
    "def log_prior(param):\n",
    "    return beta.logpdf(param, 0.5, 0.5)\n",
    "\n",
    "def log_posterior(param):\n",
    "    return log_likelihood(param) + log_prior(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3) Plot the posterior probability density functions obtained with the MCMC approximation and the analytical solution.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "\n",
    "A farmer owns a huge amount of cows. This year he ran an experiment  where he gave 10 cows medicine A and 10 medicine B and then measured whether they got sick (0) or not (1) during the summer season. Here is the resulting data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cowA = np.array([0, 1, 0, 0, 0, 0, 1, 0, 0, 0])\n",
    "cowB = np.array([0, 0, 1, 1, 1, 0, 1, 1, 1, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1) The farmer wants to know: How effective are the drugs? What is the evidence that medicine A is better or worse than medicine B ?**\n",
    "\n",
    "Hint: The outcome for each cow taking medecine A can be modeled as a random variable following a Bernouilli distribution with parameter $\\theta_1$. Similarly, the outcome for each cow taking medecine B can be modeled as a random variable following a Bernouilli distribution with parameter $\\theta_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The farmer has a huge number of cows. Earlier this year he ran an experiment where he gave 10 cows a special diet that he had heard could make them produce more milk. He recorded the number of liters of milk from these “diet” cows and from 15 “normal” cows during one month. This is the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "diet_milk = np.array([651., 679., 374., 601., 401., 609., 767., 709., 704., 679.])\n",
    "normal_milk = np.array([798., 1139., 529., 609., 553., 743., 151., 544., 488., 555., 257., 692., 678., 675., 538.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2) The farmer now wants to know: Was the diet any good, does it results in better milk production? **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: The most common approach here would be to model the milk production of each cow as a normal distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The farmer also has chickens. He tries different diets on them too with the hope that they will produce more eggs. Below is the number of eggs produced in one week by chickens on a diet and chickens eating normal chicken food:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "diet_eggs = np.array([6, 4, 2, 3, 4, 3, 0, 4, 0, 6, 3])\n",
    "normal_eggs =  np.array([4, 2, 1, 1, 2, 1, 2, 1, 3, 2, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3) The farmer now wants to know: Was the diet any good, does it result in the chickens producing more eggs ? ** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The farmer is now wondering whether the amount of time a cow spends outside in the sunshine affects how much milk she produces. To test this he makes a controlled experiment where he picks out 20 cows and assigns each a number of hours she should spend outside each day. The experiment runs for a month and Jöns records the number of liters of milk each cow produces. The data is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "milk = np.array([685, 691, 476, 1151, 879, 725, 1190, 1107, 809, 539, 298, 805, 820, 498, 1026, 1217, 1177, 684, 1061, 834])\n",
    "hours = np.array([3, 7, 6, 10, 6, 5, 10, 11, 9, 3, 6, 6, 3, 5, 8, 11, 12, 9, 5, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4)  Using this data on hours of sunshine and resulting liters of milk the farmer wants to know: Does sunshine affect milk production positively or negatively? **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3\n",
    "\n",
    "Weakly informative priors are especially critical when inferences are hindered with only weakly identifiable likelihoods, such as those arising from models with sparse data.\n",
    "\n",
    "To that end, let’s say that we are analyzing a small company and we want to model how much daily rainfall, x, affects daily income, y, using only a few measurements. For this study we will simulate data assuming that the company typically makes a few thousand dollars, or kilodollars (k$), per day without any rain and that a heavy rainfall of a few centimeters per day can severely curtail income."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8689176  1.1808266  1.89158407 1.23102573 0.5816685 ]\n",
      "[ 0.40355738 -0.90128158  1.52170117  1.66406968  1.87581803]\n"
     ]
    }
   ],
   "source": [
    "alpha = 1    # k$\n",
    "beta = -0.25 # k$ / cm\n",
    "sigma = 1    # k$\n",
    "\n",
    "N = 5\n",
    "rain = 2.*np.random.rand(N) # cm\n",
    "daily_income = np.random.normal(loc=beta * x + alpha, scale=sigma) # k$\n",
    "\n",
    "print(rain)\n",
    "print(daily_income)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming that the typical values of both rainfall and income are sufficiently large, we can ignore the fact that they are positive quantities and model their relationship with a linear regression. We can then fit this linear regression in Stan using a very long Markov chain to ensure precise quantification of our posterior distribution.\n",
    "\n",
    "**1) Fit a linear regression model using Stan and study the posterior distribution of your parameters. You will use a flat prior (this is done automatically by Stan if you don't specify any prior on your parameters.)** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weakly Informative Priors\n",
    "\n",
    "Weakly informative priors introduce scale information to regularize inferences. Scales are straightforward to reason about in applied problems, especially when units are carefully laid out, and they provide just enough information to regularize non-identified or weakly-identified likelihoods without strongly biasing the posterior away from reasonable parameter values. In order to construct weakly informative priors we need to first decompose our model into components, define default values, identify scales, then choose an explicit shape for our prior.\n",
    "\n",
    "We cannot define scales, let alone reason about them, until we first decompose our model into interpretable components. In other words, we need to find a parameterization of our model where the parameters are particularly meaningful. The parameterization we have used in our linear regression, for example, is ideal as the intercept, slope, and measurement variability have intuitive interpretations: the intercept, $\\alpha$, determines the base income without any rainfall, the slope, $\\beta$, controls how a change in rainfall affects income, and the measurement variation, $\\sigma$, quantifies the natural variability of daily income.\n",
    "\n",
    "## Weakly Informative Priors Under Well-Chosen Scales\n",
    "\n",
    "### Light-Tailed Weakly Informative Priors\n",
    "\n",
    "When the scales are well-chosen all weakly informative priors behave similarly, regularizing the posterior by penalizing extreme parameter values. The exact shape of a weakly informative prior, however, does introduce some important differences in how strong that regularization is. Let’s first consider a relatively light-tailed weakly informative prior that utilizes Gaussian distributions. Because we simulated the data already in natural units, the weakly informative priors are given simply by unit-scale Gaussians."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2) Fit a linear regression model using Stan. You will use a standard Gaussian as prior for all the parameters.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3) Plot the histogram for the posterior distribution of the parameters. Compare with the ground truth. What do you observe compared to question 1) ? **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heavy-Tailed Weakly Informative Priors\n",
    "\n",
    "To contrast, let’s now consider the more heavily-tailed priors given by Cauchy distributions. Once again our prescient choice of units admits unit-scale distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4) Fit a linear regression model using Stan. You will use a standard Cauchy as prior for all the parameters.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5) Plot the histogram for the posterior distribution of the parameters. Compare with the ground truth. What do you observe compared to question 3) ? **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weakly Informative Priors Under Poorly-Chosen Scales\n",
    "\n",
    "The differing behavior of weakly informative priors with different shapes becomes particularly striking when the scales are poorly-chosen, and the likelihood strongly favors parameter values that conflict with the priors.\n",
    "\n",
    "To demonstrate this difference, let’s simulate a larger data set where the true intercept is above the scale of our chosen units. How this tension between the weakly informative prior and the likelihood manifests in the posterior is extremely sensitive to the exact shape of the weakly informative prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.93558116 0.24031392 1.73401905 0.47031533 0.09843725 1.24823894\n",
      " 0.69594907 1.74843803 0.16011238 0.42924275]\n",
      "[11.4851514   9.42848573  8.23247025  9.36580534 10.98259069 10.86842408\n",
      "  8.8595824  10.08770919 10.50106175  9.81588199]\n"
     ]
    }
   ],
   "source": [
    "alpha = 10   # k$\n",
    "beta = -0.25 # k$ / cm\n",
    "sigma = 1    # k$\n",
    "\n",
    "N = 10\n",
    "rain = 2.*np.random.rand(N) # cm\n",
    "daily_income = np.random.normal(loc=beta * rain + alpha, scale=sigma) # k$\n",
    "\n",
    "print(rain)\n",
    "print(daily_income)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Failure Mode of Light Tails\n",
    "\n",
    "Let’s first consider the relatively light tail of a Gaussian prior distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6) Fit a linear regression model using Stan. You will use a standard Gaussian as prior for all the parameters.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7) Plot the histogram for the posterior distribution of the parameters. Compare with the ground truth. What do you observe compared to question 3) ? **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heavy-Tailed Weakly Informative Priors\n",
    "\n",
    "The heavier tail of the Cauchy prior responds very differently to a poorly-chosen scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8) Fit a linear regression model using Stan. You will use a standard Cauchy as prior for all the parameters.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9) Plot the histogram for the posterior distribution of the parameters. Compare with the ground truth. What do you observe compared to question 7) ? **"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.6",
   "language": "python",
   "name": "python3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 1,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
